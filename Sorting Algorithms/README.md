## Bubble Sort

_The bubble sort algorithm is a sorting algorithm that sorts items out by swapping smaller items for larger ones. It works by iterating through the items, checking for the larger and swapping them while **bubbling** the larger items to the end of the array. The time complexity for a bubble sort algorithm is O(n<sup>2</sup>), so it really isn't that efficient but it's a real fundamental. After each iteration, the largest item is bubbled to the end of the array, leaving the end of the array sorted - reducing the max unsorted index. Once the max unsorted index is 0, the array can be deemed to be sorted_

## Selection Sort

_The Selection sort algorithm works in a way similar to the bubble sort, it has the same time complexity but swapping is reduced and done once per inner iteration which automatically increases the chances of it performing better than bubble sort. It starts by assuming the first element in the array to be the highest, the last element in the array is termed the lastUnsortedIndex. If loops through each element and checks if the elements are larger than the first, with this, after the iteration, the largest index would be known at the end of the array. Once this is decided, we swap the element at the largest index with the element at the last unsorted index. With this, the element at the lastUnsortedIndex is now sorted having the largest element found during iteration. This effectively decreases the lastUnsortedIndex and after subsequent iterations, the array becomes completely sorted_

## Insertion Sort

_The insertion sort takes a different approach by sorting the array from the right to left. It recognises the element at the start of the array to be part of a "sorted partition", the element at index 1 is the first unsorted idnex and each element is taken and compared to the elements inside the sorted partition. When an appropriate position is found (left <=) (right >), elements in the sorted partition are shifted to the right increasing the sorted partition and reducing the unsorted partition. This goes on until the final element is shifted into it's own position._

## Shell Sort

_The Shell sort algorithm is a variation of the insertion sort algorithm. It works with switching smaller values with larger ones towards the right of the array but in this usecase, it deals with a much larger gap value instead of 1. The gap value is usually derived by dividing the length of the array by 2. Setting the origin of the iteration to the element located at gap position, and comparing it with the element that's gap positions behind. Comparisons lead to switching if the element in front is smaller and larger elements are moved towards the right. After one iteration, the gap is further divided by 2 and this goes on until the gap value hits 1 and an insertion sort is essentially done. The time complexity for the worst case scenario is O(n squared). It is an in-place (no extra memory) and unstable algorithm_

## Recursion

_Recursion refers to the concept of a method calling itself. Usually, when a method calls itself, that call is added to the call stack and is resolved down the call stack. The returned values are then propagated back to the originally calling method. This concept is very useful when iterative operations are to be improved._

## Merge Sort

_The Merge Sort algorithm, also known as the **divide and conquer** algorithm is a recursive algorithm that takes the input array and divides it recursively at the mid-section until we have a bunch of one-element arrays. In the order in which the splitting/dividing was done, the elements are merged again - but this time they are sorted. The recursive tree for this algorithm divides the array at the mid section and returns with a left and right section, this left and right sections are further subdivided and at the point where one-element arrays are left, the merging begins. To merge these elements, we perform a check on the first element of the left array to see if it's less than the first element in the right array, if it is, we move the first element of L array to a temporary array and increment the counter for the L array by one, we compare again with the R array, and based on the side that has the smallest element, we increment the counter for that side of the array until we exhaust one of both sides (no values to compare to anymore). In this case, we copy over the pending elements into the end of the temporary array, then copy the temporary array into the original input array and it turns out to be sorted!. The time complexity for this algorithm is O(nlogn)_

## Quick Sort

_This sorting algorithm is a variation of the Merge sort as it uses the same recursion and divide-and-conquer techniques to perform the sorting. The difference between this and the merge sort algorithm is that it uses a pivot element to split the array, the pivot is usually selected as the first element in the partition. After partitioning the array, we place the pivot at a location where every element before the pivot is less than the pivot and every element after the pivot is greater than the pivot. With this, there are two sides of the partition at first and this is further split for these two sides and the process goes on and on until we have divided the array into one-element arrays. The worst case time complexity for this algorithm is O(n log n) and is an unstable algorithm_